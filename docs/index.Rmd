---
title: "Linear model predictions and marginal effects with two confidence intervals"
author: "Maxime Blanchard"
date: "Last updated `r Sys.Date()`"
output: html_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This tutorial shows you how to plot predictions and marginal effects of ordinary least squares (OLS) regression results using R. It specifically shows how to plot two confidence intervals around your predictions and marginal effects to provide more information on the uncertainty of your results. Putting multiple measures of uncertainty around model estimates is becoming increasingly common. It is often done when presenting coefficients plots or point estimates of treatment effects but remains less common when plotting model predictions and marginal effects. This is somewhat surprising given how simple it is to do so. This tutorial provides a demonstration.

### Predictions

First, let's load the necessary packages. We will use the `tidyverse`[@tidyverse] suite of packages to manipulate and plot data, along with the `marginaleffects`[@marginaleffects] package to quickly generate model predictions and compute marginal effects.

```{r, message = FALSE}
library(tidyverse)
library(marginaleffects)
```


Let's start by simulating some data. We will set a seed to ensure replicability, then create the object `n` to set the number of observations we want to create and finally simulate the data. To do so, we create two random normal distributions, `x1` and `x2`, which will serve as our covariates in the model we will soon estimate. Then we build our outcome variable, which we conveniently name `outcome`, as a function of our two covariates plus some random noise.

```{r}
set.seed(35)
n <- 100
data <- data.frame(x1 = rnorm(n, 5, 5),
                   x2 = rnorm(n, -3, 10)) %>% 
    mutate(outcome = 0.4*x1 + 2*x2 + rnorm(n, 2, 13))
```

We therefore define the data-generating process (DGP) as 

$$
Y = 2 + .4 \times X_{1} + 2 \times X_{2} + \varepsilon
$$

where 

$$\varepsilon \sim \text{N}(0, 13^2 = 169)$$

and use our DGP to create 100 observations. The intercept, whose value is 2, is not explicitly stated in our call to `mutate`, but it is implied by our inclusion of a noise parameter following a normal distribution centered at 2.

Let's visualize the relationship between our two covariates and our outcome variable, reshaping the dataframe with `pivot_longer` so we can show both distributions in a single plot:

```{r}
data %>% 
    pivot_longer(cols = c(x1, x2),
                 names_to = "variable") %>% 
    ggplot(aes(x = value, y = outcome)) +
    geom_point() +
    labs(title = "Relationship between the covariates and the outcome variable",
         x = "Value", y = "Outcome") +
    theme_classic() +
    facet_wrap(. ~ variable) +
    theme(plot.title = element_text(face = "bold"))
```

As you can see, the association between `x2` and the `outcome` variable is much stronger than the latter's association with `x1`. This reflects the DGP we stipulated above.

Now we will regress our outcome variable on our two covariates, and look at the results using the `summary` function. Notice how I explicitly state the intercept, calling `1` in the regression formula. This is not necessary, as `lm` includes an intercept by default, by I personally like to be as explicit as possible when formulating my models. I find this practice to be helpful when working with complex models.

```{r}
reg <- lm(outcome ~ 1 + x1 + x2,
          data = data)
summary(reg)
```

The results we obtain are roughly reflective of the DGP we specified, although we have a sizable amount of sampling variance that comes into play given our relatively small sample size ($n = 100$). Accordingly, our estimates of the relationship between `x1` and `x2` are quite close to their "true" values -- as defined above -- which stand at .4 and 2, respectively. The point estimate of our intercept is slightly off, but it is also imprecisely estimated, as indicated by its large standard-error.

Now let's say you want to visually present those regression results, including a measure of the uncertainty of your point estimates, but do not want to arbitrarily select a given $p$-value. Typically, we are taught to focus on $p<.05$ -- i.e., 95% confidence intervals -- but there is nothing magical about this number, it is as arbitrary as $p<.06$ or $p<.029$. In fact, applied statisticians are being increasingly vocal in their calls to abandon such arbitrary thresholds of statistical significance [@gelman2006difference; @wasserstein2019moving].

But how can you do so when visually plotting regression results? One would certainly not want to abandon _any_ kind of measure of uncertainty around our estimates, that would be counter-productive. An approach which is becoming increasingly common is to plot two confidence intervals (rather than one) around our model estimates. Let's use the package `marginaleffects` to do so.

We'll focus first on presenting model-based predictions. To do so, we need to call the function `predictions()`, inside which we can specify the model object on which our predictions will be based and the confidence interval that we want around our predictions. Unfortunately, `predictions()` does not currently accommodate multiple confidence intervals, as you can see below:

```{r, error=TRUE}
# does not work
predictions(model = reg,
            conf_level = c(0.9, 0.99))
```

There's an easy work-around though, as `predictions()` gives us the standard-error around the predictions it generates. Here's an example, focusing on the variable `x1`. We simply call `newdata`, and inside `datagrid`, we specify that we want predictions from the minimum value of `x1` to its maximum value, with a prediction for each 0.1 increment:

```{r}
preds_x1 <- predictions(model = reg,
                        newdata = datagrid(x1 = seq(min(data$x1), max(data$x1), by = 0.1)))
```

This gives us a dataframe of predicted values at each value of the `x1` variable, with additional information on each prediction:

```{r}
head(preds_x1)
```

By moving up in 0.1 increments, we now have a pretty long list of predictions, which ensures the precision of our visual display of the relationship between `x1` and `outcome`:

```{r}
nrow(preds_x1)
```

Using the dataframe created above, we can plot this relationship with two confidence intervals, simply by calling `geom_ribbon` twice, with a different critical value for each confidence band (here I went with a 90% and a 99% confidence interval):

```{r}
ggplot(preds_x1, aes(x = x1, y = predicted)) +
    geom_ribbon(aes(ymin = predicted - 2.576*std.error, ymax = predicted + 2.576*std.error), 
                fill = "grey70", alpha = 0.3) +
    geom_ribbon(aes(ymin = predicted - 1.645*std.error, ymax = predicted + 1.645*std.error), 
                fill = "grey30", alpha = 0.3) +
    geom_line() +
    labs(y = "Predicted value") +
    theme_classic()
```

Make sure to vary the ribbon colors using `fill` to make the two confidence intervals distinguishable from one another. Also, for aesthetic purposes, I personally like to use `alpha` to make the ribbons transparent a little bit.

The same approach works for `x2`:

```{r}
preds_x2 <- predictions(model = reg,
                        newdata = datagrid(x2 = seq(min(data$x2), max(data$x2), by = 0.1)))

ggplot(preds_x2, aes(x = x2, y = predicted)) +
    geom_ribbon(aes(ymin = predicted - 2.576*std.error, ymax = predicted + 2.576*std.error), 
                fill = "grey70", alpha = 0.3) +
    geom_ribbon(aes(ymin = predicted - 1.645*std.error, ymax = predicted + 1.645*std.error), 
                fill = "grey30", alpha = 0.3) +
    geom_line() +
    labs(y = "Predicted value") +
    theme_classic()
```

Using the piping operator along with `facet_wrap`, we can plot both results in a single plot and avoid having to create unnecessary objects to maximize the efficiency of our code. Essentially, I create predictions for `x1` via a first call to `predictions()`, then I create a new column called `var` that will allow me to distinguish between predictions for `x1` and predictions for `x1` and then I use `select()` to keep only the columns I may need. I then do the same for `x2` and combine together my two sets of predictions using `bind_rows()`. Then I do some reshaping with `pivot_longer()` that results in some NA rows that I remove with `drop_na()`. Finally, I plot the data using `ggplot`.

```{r}
bind_rows(
    predictions(model = reg,
                newdata = datagrid(x1 = seq(min(data$x1), max(data$x1), by = 0.1))) %>% 
        mutate(var = "x1") %>% 
        dplyr::select(rowid, predicted, std.error, x1),
    predictions(model = reg,
                newdata = datagrid(x2 = seq(min(data$x2), max(data$x2), by = 0.1))) %>% 
        mutate(var = "x2") %>% 
        dplyr::select(rowid, predicted, std.error, x2)
    ) %>% 
    # reshaping with pivot_longer to have both variables in a single column
    pivot_longer(cols = c(x1, x2),
                 names_to = "variable") %>% 
    # binding the rows creates some NAs, as each binded dataframe has a unique column (x1 and x2)
    # let's remove these unnecessary rows using drop_na()
    drop_na() %>% 
    ggplot(aes(x = value, y = predicted)) +
    geom_ribbon(aes(ymin = predicted - 2.576*std.error, ymax = predicted + 2.576*std.error), 
                fill = "grey70", alpha = 0.3) +
    geom_ribbon(aes(ymin = predicted - 1.645*std.error, ymax = predicted + 1.645*std.error), 
                fill = "black", alpha = 0.3) +
    geom_line() +
    labs(title = "Predicted values of the outcome conditional on the value of each covariate",
         subtitle = "Based on OLS results",
         x = "Value of covariate", y = "Predicted value") +
    theme_classic() +
    theme(plot.title = element_text(face = "bold")) +
    facet_wrap(. ~ variable,
               scales = "free_x") # since both variables have different scales
```

I let the scales vary for the x-axis since the two variables are on different scales. And there's how to create predictions plots with two confidence intervals.

### Marginal effects

We can use a similar approach to present the marginal effect of `x1` and `x2`. While prediction plots are very useful to convey information about the substantive significance of relationships, marginal effects provide direct information on their statistical significance.

This time, we call the `marginaleffects()` function, and then summarize the information with the help of `tidy()`, which gives us a simple dataframe of marginal effects whose length is equal to the number of covariates in the model - in our case, only two.

```{r}
marginaleffects(model = reg) %>% 
    tidy()
```

In a simple case like the one we have here, marginal effects provide the same information as regression results. But in more complex models, notably models with interaction terms (covered below), they provide information that tabled results alone cannot provide. Even in simple cases like ours, they allow to generate coefficients plots, which are progressively replacing regression tables as they are easier to interpret.

To plot our marginal effects, we can use a similar approach as we did previously, using the piping operator to directly plug the dataframe above inside `ggplot()` without creating any object. This time, since marginal effects are point estimates, we need to call `geom_point` and add two error bars around it (one for each confidence interval we want to plot, here 90% and 99% confidence intervals). Each error bar needs to be distinguishable from the other in some way. Here, I picked two different colors (90% CI = black, 99% CI = grey) and also vary the size of the error bars to make it easier to distinguish between the two confidence levels. I also add a line at zero since this is typically the value against which we want to compare our point estimates. Finally, I flip the plot horizontally using `coord_flip`, as I find that this makes a more aesthetic display of the results and use the `labs` argument to put useful information to understand the plot.

```{r}
marginaleffects(model = reg) %>% 
    tidy() %>% 
    ggplot(aes(x = term, y = estimate)) +
    geom_hline(yintercept = 0) + # to compare our point estimates to the null hypothesis of zero effect
    geom_point(size = 3.5) +
    geom_errorbar(aes(ymin = estimate - 2.576*std.error, ymax = estimate + 2.576*std.error),
                  width = 0, size = 1, color = "grey70") +
    geom_errorbar(aes(ymin = estimate - 1.645*std.error, ymax = estimate + 1.645*std.error),
                  width = 0, size = 1.5) +
    labs(title = "Marginal effects of covariates",
         subtitle = "Based on OLS results",
         caption = "\nMarginal effects with 90% (black) and 99% (grey) confidence intervals",
         x = "Covariate", y = "Estimate") +
    theme_classic() +
    theme(plot.title = element_text(face = "bold")) +
    coord_flip()
```

This is an interesting case where having two confidence intervals would nuance our interpretation of the results for `x1`, as its 90% confidence interval does not overlap zero, but is 99% confidence interval does. This is useful information about the statistical significance of the relationship between `x1` and `outcome` that neither prediction plots nor standard regression tables convey, as the latter typically only present standard-errors along with stars indicating a given level of statistical significance.

The literature on applied statistics is progressively converging toward an understanding of $p$-values as continuous measures of the uncertainty of our regression results, at the expense of a more traditional understanding of them as binary thresholds determining statistical significance. Yet, how exactly should we interpret results such as that for `x1`, which is statistically significant at $p<.1$ but not at $p<.01$, is open for interpretation. Indeed, this greater embrace of uncertainty also comes at a cost, the cost being that there is no strictly right or wrong way to interpret the results. The interpretation may become a bit fuzzier, but in my humble opinion it is a price well worth paying as using continuous measures of uncertainty is considerably more informative than the traditional binary conception of statistical uncertainty.

### Marginal effects of interaction terms

With interaction terms, plotting marginal effects is a bit trickier as the marginal effect of each constituent term is conditional on the value of the other constituent term. Let's dig into this a little bit. First, we'll start by modifying our dataframe of simulated values to add an interaction between `x1` and `x2` in the DGP.

```{r}
set.seed(12)
n <- 100
data <- data.frame(x1 = rnorm(n, 5, 5),
                   x2 = rnorm(n, -3, 10)) %>% 
    mutate(outcome = 0.4*x1 + 2*x2 - 0.3*(x1*x2) + rnorm(n, 2, 13))
```

So having done this, we know that the marginal effect of both covariates is conditional on the value of the other covariate. Our new DGP can be represented as

$$
Y = 2 + 0.4 \times X_{1} + 2 \times X_{2} - 0.3 \times (X_{1} \times X_{2}) + \varepsilon
$$

so the marginal effect of `x1` is

$$
\frac{\partial Y}{\partial X_{1}} = 0.4 - 0.3 \times X_{2}
$$

and the marginal effect of `x2` is

$$
\frac{\partial Y}{\partial X_{2}} = 2 - 0.3 \times X_{1}.
$$

But how can we represent this visually? First, let's run a new regression model, this time interacting `x1` and `x2`:

```{r}
reg_int = lm(outcome ~ 1 + x1 + x2 + x1:x2,
             data = data)
summary(reg_int)
```

<!-- Now the variance of the marginal effect of `x1` can be represented as -->

<!-- $$ -->
<!-- \mathbb{V} \left[ \frac{\partial Y}{\partial X_{1}} \right] = \mathbb{V} [\beta_{1}] + X_{2}^{2} \times \mathbb{V}[\beta_{3}] + 2 \times X_{2} \times Cov[\beta_{1}, \beta_{3}] -->
<!-- $$ -->

<!-- and that of `x2` as -->

<!-- $$ -->
<!-- \mathbb{V} \left[ \frac{\partial Y}{\partial X_{2}} \right] = \mathbb{V} [\beta_{2}] + X_{1}^{2} \times \mathbb{V}[\beta_{3}] + 2 \times X_{1} \times Cov[\beta_{2}, \beta_{3}]. -->
<!-- $$ -->

Now to plot the marginal effect of each variable, we once again need to work with the standard-error of its marginal effect, as above. So we will use a similar approach as we did above, but this time we need to generate our marginal effects over a range of values given that the marginal effect of `x1` is conditional on the value of `x2`, and vice versa. So for each variable, we'll need to compute its marginal effect over the full range of its conditioning variable. The function `plot_cme()` from `marginaleffects` allows us to easily create conditional marginal effects plots -- which is what we're trying to do here -- but once again it does not accommodate more than a single confidence interval, so we'll have to do it on our own. Fortunately, it is easy to do so.

Let's start by looking at the marginal effect of `x1` conditional on the value of `x2`. To create conditional marginal effects plots, we need to specify inside `datagrid` the values of the conditioning variable for which we want marginal effects. So in our case, we ask `marginaleffects()` to give us marginal effects over the full range of the `x2` variable. Doing so will create a dataframe of marginal effects for every model parameter, but since we only care about `x1` here, we'll use `filter()` to keep only rows that present the marginal effect of `x1`:

<!-- Ok, so what? Well, knowing that we can manually compute the any confidence interval around our marginal effect. For `x1`: -->

<!-- $$ -->
<!-- \frac{\partial Y}{\partial X_{1}} \pm t_{\frac{\alpha}{2}}SE \biggl( \frac{\partial Y}{\partial X_{1}} \biggr). -->
<!-- $$ -->

<!-- For `x2`: -->

<!-- $$ -->
<!-- \frac{\partial Y}{\partial X_{2}} \pm t_{\frac{\alpha}{2}}SE \biggl( \frac{\partial Y}{\partial X_{2}} \biggr). -->
<!-- $$ -->

```{r}
marginaleffects(reg_int,
                newdata = datagrid(x2 = seq(min(data$x2), max(data$x2), by = 1))) %>% 
    filter(term == "x1") %>% 
    head()
```

Toward the right end of the table, you can see the columns x1 and x2, which indicate the values of `x1` and `x2` for which marginal effects were computed. As we can see, `x2` varies according to the sequence that we specified in `datagrid`, whereas `x1` is held at its mean, which is the default in `marginaleffects()` for variables that weren't explicitly specified. This does not change anything to our predictions, as the partial derivative of `x1` is only conditional on the value of `x2`, not of `x1` itself.

Now let's put this in a plot:

```{r}
marginaleffects(reg_int,
                newdata = datagrid(x2 = seq(min(data$x2), max(data$x2), by = 1))) %>% 
    filter(term == "x1") %>% 
    ggplot(aes(x = x2, y = dydx)) +
    geom_hline(yintercept = 0) + # to assess statistical significance
    geom_ribbon(aes(ymin = dydx - 2.576*std.error, ymax = dydx + 2.576*std.error),
                fill = "grey70", alpha = 0.3) +
    geom_ribbon(aes(ymin = dydx - 1.645*std.error, ymax = dydx + 1.645*std.error),
                fill = "grey30", alpha = 0.3) +
    geom_line() +
    labs(y = "Predicted value") +
    theme_classic()
```

I like to add a reference line at zero to make it easier to assess statistical significance and also to see when an effect turns negative or positive.

Let's do the same for each of our two covariates, using `bind_rows()` as I detail above and using `mutate()` to create a new variable called `quantity` that will serve as an elegant title to indicate the quantity of interest plotted in each facet:

```{r}
bind_rows(
    marginaleffects(reg_int,
                    newdata = datagrid(x2 = seq(min(data$x2), max(data$x2), by = 1))) %>% 
        filter(term == "x1") %>% 
        mutate(conditioning = x2) %>% 
        dplyr::select(rowid, term, dydx, std.error, x2, x1, conditioning),
    marginaleffects(reg_int,
                    newdata = datagrid(x1 = seq(min(data$x1), max(data$x1), by = 1))) %>% 
        filter(term == "x2") %>% 
        mutate(conditioning = x1) %>% 
        dplyr::select(rowid, term, dydx, std.error, x2, x1, conditioning)
) %>% 
    mutate(quantity = case_when(
        term == "x1" ~ "Marginal effect of x1",
        term == "x2" ~ "Marginal effect of x2"
    )) %>% 
    ggplot(aes(x = conditioning, y = dydx)) +
    geom_hline(yintercept = 0) + # to assess statistical significance
    geom_ribbon(aes(ymin = dydx - 2.576*std.error, ymax = dydx + 2.576*std.error),
                fill = "grey70", alpha = 0.3) +
    geom_ribbon(aes(ymin = dydx - 1.645*std.error, ymax = dydx + 1.645*std.error),
                fill = "grey30", alpha = 0.3) +
    geom_line() +
    labs(title = "Marginal effect of x1 and x2",
         x = "x2                                                                               x1",
         y = "Predicted value") +
    theme_classic() +
    theme(plot.title = element_text(face = "bold")) +
    facet_wrap(. ~ quantity,
               scales = "free_x") # since conditioning variables have different scales
```

And there's our plot. One issue I faced when creating this plot is the impossibility to have different axis titles when using facets. Given that both plots condition on a different variable and that I wanted the conditioning variable to be named explicitly, I needed to have different x-axis titles. The solution I found is to simply write both axis titles in `labs()` as a common axis name, but put enough spaces between them so that they look like different axis titles in the plot. It's not elegant, but it works. Other packages could have been used to create a similar plot, but I like to stick with `tidyverse` function as much as possible.

So there it is, even for interactions it's quite easy to plot multiple confidence intervals around your marginal effects. I decided to have two confidence intervals (90% and 99%), but you can obviously plot more than two and pick other $\alpha$ levels, these are all arbitrary decisions.

```{r, eval=FALSE, echo = FALSE}
# this is just to create the image for the website

bind_rows(
    predictions(model = reg,
                newdata = datagrid(x1 = seq(min(data$x1), max(data$x1), by = 0.1))) %>% 
        mutate(var = "x1") %>% 
        dplyr::select(rowid, predicted, std.error, x1),
    predictions(model = reg,
                newdata = datagrid(x2 = seq(min(data$x2), max(data$x2), by = 0.1))) %>% 
        mutate(var = "x2") %>% 
        dplyr::select(rowid, predicted, std.error, x2)
    ) %>% 
    # reshaping with pivot_longer to have both variables in a single column
    pivot_longer(cols = c(x1, x2),
                 names_to = "variable") %>% 
    # binding the rows creates some NAs, as each binded dataframe has a unique column (x1 and x2)
    # let's remove these unnecessary rows using drop_na()
    drop_na() %>% 
    ggplot(aes(x = value, y = predicted)) +
    geom_ribbon(aes(ymin = predicted - 2.576*std.error, ymax = predicted + 2.576*std.error), 
                fill = "grey70", alpha = 0.3) +
    geom_ribbon(aes(ymin = predicted - 1.645*std.error, ymax = predicted + 1.645*std.error), 
                fill = "black", alpha = 0.3) +
    geom_line() +
    labs(x = "Value of covariate", y = "Predicted value") +
    theme_classic() +
    theme(plot.title = element_text(face = "bold")) +
    facet_wrap(. ~ variable,
               scales = "free_x") # since both variables have different scales
ggsave("./plot.png", width = 8, height = 3.7)
```

## References

